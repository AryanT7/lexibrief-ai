# Model configuration
model:
  name: "mistralai/Mistral-7B-Instruct-v0.2"
  max_length: 1024  # Reduced to save memory

# Training configuration
training:
  seed: 42
  num_train_epochs: 3
  per_device_train_batch_size: 1  # Reduced for CPU
  per_device_eval_batch_size: 1   # Reduced for CPU
  gradient_accumulation_steps: 8   # Increased to compensate for smaller batch size
  learning_rate: 2e-4
  warmup_ratio: 0.03
  weight_decay: 0.01
  logging_steps: 10
  eval_steps: 50                  # Reduced frequency for CPU
  save_steps: 50                  # Reduced frequency for CPU
  max_grad_norm: 0.3
  save_total_limit: 2             # Reduced to save disk space
  evaluation_strategy: "steps"
  save_strategy: "steps"
  load_best_model_at_end: true
  metric_for_best_model: "rouge-l"
  greater_is_better: true

# QLoRA configuration
peft:
  r: 32                          # Reduced for memory efficiency
  lora_alpha: 16
  lora_dropout: 0.1
  bias: "none"
  task_type: "CAUSAL_LM"
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
  inference_mode: false

# Data configuration
data:
  train_size: 0.9
  validation_size: 0.1

# Logging and output
output:
  output_dir: "outputs"
  logging_dir: "logs"
  report_to: "none"              # Disabled wandb for simplicity
  project_name: "LexiBrief"

# Hardware configuration
hardware:
  mixed_precision: "no"          # Changed to no mixed precision for CPU
  device: "cpu"                  # Set to CPU explicitly 