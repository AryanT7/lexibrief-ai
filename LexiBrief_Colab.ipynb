{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# LexiBrief: AI-Powered Legal Document Summarizer\n",
        "\n",
        "# This Colab notebook sets up and runs the LexiBrief project, which uses the Mistral-7B-Instruct model for legal document summarization. The notebook will:\n",
        "\n",
        "# 1. Set up the environment and dependencies\n",
        "# 2. Clone the project from GitHub\n",
        "# 3. Configure the model for CPU/GPU training\n",
        "# 4. Train and evaluate the model\n",
        "# 5. Launch the Gradio interface for testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if we're running in Colab\n",
        "import sys\n",
        "import os\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"Setting up Colab environment...\")\n",
        "\n",
        "    # Pin websockets to match Gradio\n",
        "    !pip install -q \"websockets>=10.0,<12.0\"\n",
        "    \n",
        "    # Install Gradio first to lock its dependencies\n",
        "    !pip install -q \"gradio==3.40.1\"\n",
        "\n",
        "    # Install remaining project-specific dependencies\n",
        "    !pip install -q transformers pandas numpy tqdm \\\n",
        "        peft datasets accelerate bitsandbytes evaluate \\\n",
        "        rouge_score nltk wandb python-dotenv requests PyYAML \\\n",
        "        scipy sentencepiece\n",
        "\n",
        "    # Clone or pull repo\n",
        "    if not os.path.exists('lexibrief-ai'):\n",
        "        print(\"\\nCloning LexiBrief repository...\")\n",
        "        !git clone https://github.com/AryanT7/lexibrief-ai.git\n",
        "    else:\n",
        "        print(\"\\nRepository already exists, updating...\")\n",
        "        %cd lexibrief-ai\n",
        "        !git pull\n",
        "        %cd ..\n",
        "\n",
        "    # Confirm GPU\n",
        "    print(\"\\nVerifying GPU setup...\")\n",
        "    import torch\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
        "    else:\n",
        "        print(\"No GPU found. Using CPU.\")\n",
        "else:\n",
        "    print(\"Running in local environment\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fix numpy conflict\n",
        "!pip uninstall -y numpy\n",
        "!pip install -q numpy==1.26.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up broken install\n",
        "!pip uninstall -y transformers\n",
        "!rm -rf /usr/local/lib/python3.11/dist-packages/transformers\n",
        "\n",
        "!pip install -q transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import sys\n",
        "import yaml\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import gradio as gr\n",
        "\n",
        "# Add the project root to Python path\n",
        "sys.path.append('.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure training settings based on available hardware\n",
        "def setup_training_config():\n",
        "    config_updates = {\n",
        "        'model': {\n",
        "            'name': 'mistralai/Mistral-7B-Instruct-v0.1',\n",
        "            'load_in_4bit': True,\n",
        "            'use_flash_attention': True\n",
        "        },\n",
        "        'training': {\n",
        "            'per_device_train_batch_size': 4 if torch.cuda.is_available() else 1,\n",
        "            'per_device_eval_batch_size': 4 if torch.cuda.is_available() else 1,\n",
        "            'gradient_accumulation_steps': 4,\n",
        "            'num_train_epochs': 3,\n",
        "            'learning_rate': 2e-4,\n",
        "            'max_grad_norm': 0.3,\n",
        "            'warmup_ratio': 0.03\n",
        "        },\n",
        "        'lora': {\n",
        "            'r': 64,\n",
        "            'lora_alpha': 16,\n",
        "            'target_modules': [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "            'bias': \"none\",\n",
        "            'task_type': \"CAUSAL_LM\"\n",
        "        },\n",
        "        'hardware': {\n",
        "            'mixed_precision': 'fp16' if torch.cuda.is_available() else 'no',\n",
        "            'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Create configs directory if it doesn't exist\n",
        "    os.makedirs('configs', exist_ok=True)\n",
        "    \n",
        "    # Save config\n",
        "    with open('configs/training_config.yaml', 'w') as f:\n",
        "        yaml.dump(config_updates, f, default_flow_style=False)\n",
        "    \n",
        "    return config_updates\n",
        "\n",
        "# Set up the configuration\n",
        "config = setup_training_config()\n",
        "print(\"\\nTraining Configuration:\")\n",
        "print(yaml.dump(config, default_flow_style=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Login to Hugging Face\n",
        "from huggingface_hub import login\n",
        "login()\n",
        "#isithf_nkIwjkghPZIhMVdirYaTxgJQqUEGTpGVzGvJ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -U datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and prepare the dataset\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "\n",
        "def prepare_training_data():\n",
        "    print(\"Loading billsum dataset...\")\n",
        "    dataset = load_dataset(\"billsum\", download_mode=\"force_redownload\", keep_in_memory=True)\n",
        "    \n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config['model']['name'])\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    def preprocess_function(examples):\n",
        "        # Combine input and target for each example\n",
        "        model_inputs = []\n",
        "        for text, summary in zip(examples['text'], examples['summary']):\n",
        "            # Format the prompt\n",
        "            prompt = f\"Instruction: Summarize the following legal document concisely.\\n\\nDocument: {text}\\n\\nSummary: {summary}\"\n",
        "            \n",
        "            # Tokenize the full sequence\n",
        "            tokenized = tokenizer(\n",
        "                prompt,\n",
        "                truncation=True,\n",
        "                max_length=1024,\n",
        "                padding=\"max_length\",\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            \n",
        "            # Create labels (same as input_ids, -100 for prompt tokens)\n",
        "            labels = tokenized.input_ids.clone()\n",
        "            \n",
        "            # Find the position of \"Summary:\" in the tokenized input\n",
        "            prompt_tokens = tokenizer.encode(\"Summary:\", add_special_tokens=False)\n",
        "            prompt_end_pos = None\n",
        "            \n",
        "            for i in range(len(tokenized.input_ids[0]) - len(prompt_tokens)):\n",
        "                if tokenized.input_ids[0][i:i+len(prompt_tokens)].tolist() == prompt_tokens:\n",
        "                    prompt_end_pos = i + len(prompt_tokens)\n",
        "                    break\n",
        "            \n",
        "            if prompt_end_pos is not None:\n",
        "                # Mask the prompt part with -100\n",
        "                labels[0, :prompt_end_pos] = -100\n",
        "            \n",
        "            model_inputs.append({\n",
        "                'input_ids': tokenized.input_ids[0],\n",
        "                'attention_mask': tokenized.attention_mask[0],\n",
        "                'labels': labels[0]\n",
        "            })\n",
        "        \n",
        "        # Convert to batched format\n",
        "        batch = {\n",
        "            'input_ids': torch.stack([x['input_ids'] for x in model_inputs]),\n",
        "            'attention_mask': torch.stack([x['attention_mask'] for x in model_inputs]),\n",
        "            'labels': torch.stack([x['labels'] for x in model_inputs])\n",
        "        }\n",
        "        \n",
        "        return batch\n",
        "    \n",
        "    # Process datasets\n",
        "    train_dataset = dataset['train'].map(\n",
        "        preprocess_function,\n",
        "        batched=True,\n",
        "        batch_size=8,  # Process in smaller batches\n",
        "        remove_columns=dataset['train'].column_names,\n",
        "        desc=\"Processing training data\"\n",
        "    )\n",
        "    \n",
        "    eval_dataset = dataset['test'].map(\n",
        "        preprocess_function,\n",
        "        batched=True,\n",
        "        batch_size=8,  # Process in smaller batches\n",
        "        remove_columns=dataset['test'].column_names,\n",
        "        desc=\"Processing evaluation data\"\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nDataset Statistics:\")\n",
        "    print(f\"Training examples: {len(train_dataset)}\")\n",
        "    print(f\"Evaluation examples: {len(eval_dataset)}\")\n",
        "    \n",
        "    return train_dataset, eval_dataset, tokenizer\n",
        "\n",
        "# Prepare the datasets\n",
        "train_dataset, eval_dataset, tokenizer = prepare_training_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize and prepare the model for training\n",
        "def setup_model():\n",
        "    print(\"Loading base model...\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        config['model']['name'],\n",
        "        load_in_4bit=config['model']['load_in_4bit'],\n",
        "        device_map='auto',\n",
        "        torch_dtype=torch.float16 if config['hardware']['mixed_precision'] == 'fp16' else torch.float32\n",
        "    )\n",
        "    \n",
        "    # Prepare model for k-bit training\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "    \n",
        "    # Configure LoRA\n",
        "    peft_config = LoraConfig(\n",
        "        r=config['lora']['r'],\n",
        "        lora_alpha=config['lora']['lora_alpha'],\n",
        "        target_modules=config['lora']['target_modules'],\n",
        "        bias=config['lora']['bias'],\n",
        "        task_type=config['lora']['task_type']\n",
        "    )\n",
        "    \n",
        "    # Get PEFT model\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    \n",
        "    # Print trainable parameters\n",
        "    model.print_trainable_parameters()\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Set up the model\n",
        "model = setup_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up and run training\n",
        "def train_model():\n",
        "    # Create output directory\n",
        "    os.makedirs(\"./results\", exist_ok=True)\n",
        "    \n",
        "    # Set up training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        num_train_epochs=config['training']['num_train_epochs'],\n",
        "        per_device_train_batch_size=config['training']['per_device_train_batch_size'],\n",
        "        per_device_eval_batch_size=config['training']['per_device_eval_batch_size'],\n",
        "        gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],\n",
        "        learning_rate=config['training']['learning_rate'],\n",
        "        max_grad_norm=config['training']['max_grad_norm'],\n",
        "        warmup_ratio=config['training']['warmup_ratio'],\n",
        "        logging_steps=10,\n",
        "        # Evaluation and saving settings\n",
        "        eval_strategy=\"steps\",  # Evaluate every eval_steps\n",
        "        save_strategy=\"steps\",       # Save every save_steps\n",
        "        eval_steps=50,              # Evaluate every 50 steps\n",
        "        save_steps=50,              # Save every 50 steps\n",
        "        save_total_limit=2,         # Keep only the 2 best checkpoints\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"loss\",\n",
        "        greater_is_better=False,    # Lower loss is better\n",
        "        # Logging settings\n",
        "        report_to=\"none\",          # Disable wandb logging\n",
        "        # Push to hub settings\n",
        "        push_to_hub=False,\n",
        "    )\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    print(\"\\nStarting training...\")\n",
        "    trainer.train()\n",
        "    \n",
        "    print(\"\\nSaving model...\")\n",
        "    trainer.save_model(\"./final_model\")\n",
        "    \n",
        "    return trainer\n",
        "\n",
        "# Run training\n",
        "trainer = train_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the model with a sample document\n",
        "def test_model():\n",
        "    test_text = \"\"\"\n",
        "    SECTION 1. SHORT TITLE.\n",
        "    This Act may be cited as the \"Sample Legal Document Act of 2024\".\n",
        "    \n",
        "    SECTION 2. PURPOSE.\n",
        "    The purpose of this Act is to demonstrate the capabilities of the LexiBrief model\n",
        "    in summarizing legal documents effectively and accurately.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Prepare input\n",
        "    prompt = f\"Instruction: Summarize the following legal document concisely.\\n\\nDocument: {test_text}\\n\\nSummary:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "    \n",
        "    # Generate summary\n",
        "    print(\"Generating summary...\")\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.7,\n",
        "        num_return_sequences=1,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    \n",
        "    # Decode and print summary\n",
        "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(\"\\nTest Results:\")\n",
        "    print(\"Original Text:\")\n",
        "    print(test_text)\n",
        "    print(\"\\nGenerated Summary:\")\n",
        "    print(summary)\n",
        "\n",
        "# Run test\n",
        "test_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Launch the Gradio interface for interactive testing\n",
        "def create_demo():\n",
        "    def summarize(text):\n",
        "        prompt = f\"Instruction: Summarize the following legal document concisely.\\n\\nDocument: {text}\\n\\nSummary:\"\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
        "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "        \n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.7,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "        \n",
        "        summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return summary\n",
        "\n",
        "    # Create Gradio interface\n",
        "    demo = gr.Interface(\n",
        "        fn=summarize,\n",
        "        inputs=gr.Textbox(lines=10, label=\"Input Legal Document\"),\n",
        "        outputs=gr.Textbox(label=\"Generated Summary\"),\n",
        "        title=\"LexiBrief: Legal Document Summarizer\",\n",
        "        description=\"Enter a legal document and get a concise summary.\",\n",
        "        examples=[\n",
        "            [\"SECTION 1. SHORT TITLE.\\nThis Act may be cited as the 'Sample Legal Document Act of 2024'.\\n\\nSECTION 2. PURPOSE.\\nThe purpose of this Act is to demonstrate the capabilities of the LexiBrief model in summarizing legal documents effectively and accurately.\"]\n",
        "        ]\n",
        "    )\n",
        "    return demo\n",
        "\n",
        "# Launch the interface\n",
        "demo = create_demo()\n",
        "demo.launch(share=True)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
