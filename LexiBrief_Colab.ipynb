{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LexiBrief: AI-Powered Legal Document Summarizer\n",
        "\n",
        "# This Colab notebook sets up and runs the LexiBrief project, which uses the FLAN-T5 model for legal document summarization. The notebook will:\n",
        "\n",
        "# 1. Set up the environment and dependencies\n",
        "# 2. Configure the model for GPU training\n",
        "# 3. Train and evaluate the model\n",
        "# 4. Launch the Gradio interface for testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up the Colab environment\n",
        "import sys\n",
        "import os\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"Setting up Colab environment...\")\n",
        "\n",
        "    # Clean environment first\n",
        "    !pip uninstall -y numpy transformers torch torchvision torchaudio datasets accelerate -q\n",
        "    \n",
        "    # Install numpy 1.26.4 first\n",
        "    !pip install -q numpy==1.26.4\n",
        "    \n",
        "    # Install PyTorch and related packages\n",
        "    !pip install -q torch torchvision torchaudio\n",
        "    \n",
        "    # Install transformers and its dependencies\n",
        "    !pip install -q transformers==4.40.2\n",
        "    !pip install -q accelerate==0.26.0\n",
        "    !pip install -q datasets\n",
        "    !pip install -q peft==0.10.0\n",
        "    \n",
        "    # Install other dependencies\n",
        "    !pip install -q \"websockets>=10.0,<12.0\"\n",
        "    !pip install -q evaluate rouge_score nltk\n",
        "    !pip install -q wandb python-dotenv requests PyYAML scipy sentencepiece\n",
        "    !pip install -q gradio==3.40.1\n",
        "    \n",
        "    # Force reinstall numpy to ensure version\n",
        "    !pip install -q --force-reinstall numpy==1.26.4\n",
        "\n",
        "    # Confirm GPU\n",
        "    print(\"\\nVerifying GPU setup...\")\n",
        "    import torch\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
        "    else:\n",
        "        print(\"No GPU found. Using CPU.\")\n",
        "\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import sys\n",
        "import yaml\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    T5ForConditionalGeneration\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import gradio as gr\n",
        "\n",
        "# Create necessary directories\n",
        "os.makedirs('outputs', exist_ok=True)\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "# Add the project root to Python path\n",
        "sys.path.append('.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure training settings based on available hardware\n",
        "def setup_training_config():\n",
        "    # Replace with your Hugging Face username\n",
        "    HF_USERNAME = \"AryanT11\"\n",
        "    \n",
        "    config_updates = {\n",
        "        'model': {\n",
        "            'name': 'google/flan-t5-base',\n",
        "            'use_flash_attention': False,  # T5 doesn't use flash attention\n",
        "            'max_length': 384,  # Further reduced for faster processing while keeping context\n",
        "            'hub_model_id': f\"{HF_USERNAME}/lexibrief-legal-summarizer\"  # Model name on Hub\n",
        "        },\n",
        "        'training': {\n",
        "            'per_device_train_batch_size': 12,  # Increased but still stable\n",
        "            'per_device_eval_batch_size': 24,  # Doubled eval batch size\n",
        "            'gradient_accumulation_steps': 1,  # No accumulation needed\n",
        "            'num_train_epochs': 2,  # Kept at 2 epochs\n",
        "            'learning_rate': 8e-4,  # Slightly more aggressive learning\n",
        "            'max_grad_norm': 1.0,  # Keep gradient clipping\n",
        "            'warmup_ratio': 0.01,  # Minimal warmup\n",
        "            'output_dir': './outputs',\n",
        "            'final_model_dir': './models/flan-t5-legal',\n",
        "            'logging_steps': 50,  # Even less frequent logging\n",
        "            'eval_steps': 200,  # Less frequent evaluation\n",
        "            'save_steps': 200,  # Less frequent saving\n",
        "            'save_total_limit': 1,  # Keep only the best checkpoint\n",
        "            'push_to_hub': True,  # Enable pushing to Hub\n",
        "            'hub_strategy': 'end',  # Push only at the end of training\n",
        "            'hub_model_id': f\"{HF_USERNAME}/lexibrief-legal-summarizer\",  # Model name on Hub\n",
        "            'hub_private_repo': False,  # Make the model public\n",
        "        },\n",
        "        'lora': {\n",
        "            'r': 32,  # Reduced rank but still effective\n",
        "            'lora_alpha': 32,  # Increased alpha for stronger updates\n",
        "            'target_modules': [\"q\", \"k\", \"v\", \"o\"],  # T5 attention layer names\n",
        "            'bias': \"none\",\n",
        "            'task_type': TaskType.SEQ_2_SEQ_LM,  # T5 is a seq2seq model\n",
        "            'inference_mode': False\n",
        "        },\n",
        "        'hardware': {\n",
        "            'mixed_precision': 'bf16',  # Using bfloat16\n",
        "            'device_map': {'': 'cuda:0'},  # Force T4 GPU usage\n",
        "            'max_memory': {0: \"14GB\"},  # Reserve 1GB for system\n",
        "            'pin_memory': True,  # Faster data transfer to GPU\n",
        "            'dataloader_num_workers': 4  # Parallel data loading\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Create configs directory if it doesn't exist\n",
        "    os.makedirs('configs', exist_ok=True)\n",
        "\n",
        "    # Save config\n",
        "    with open('configs/training_config.yaml', 'w') as f:\n",
        "        yaml.dump(config_updates, f, default_flow_style=False)\n",
        "\n",
        "    return config_updates\n",
        "\n",
        "# Set up the configuration\n",
        "config = setup_training_config()\n",
        "print(\"\\nTraining Configuration:\")\n",
        "print(yaml.dump(config, default_flow_style=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and prepare the dataset\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "\n",
        "def prepare_training_data():\n",
        "    print(\"Loading billsum dataset...\")\n",
        "    dataset = load_dataset(\"billsum\", download_mode=\"force_redownload\", keep_in_memory=True)\n",
        "\n",
        "    # Filter out very long documents for faster training\n",
        "    def filter_long_docs(example):\n",
        "        return len(example['text'].split()) < 900  # Reduced from 1000 words\n",
        "\n",
        "    dataset = dataset.filter(\n",
        "        filter_long_docs,\n",
        "        num_proc=4  # Parallel processing\n",
        "    )\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config['model']['name'])\n",
        "\n",
        "    def preprocess_function(examples):\n",
        "        # Format inputs for T5\n",
        "        inputs = [\n",
        "            f\"summarize legal document: {text}\"\n",
        "            for text in examples['text']\n",
        "        ]\n",
        "        \n",
        "        # Tokenize inputs with dynamic padding\n",
        "        model_inputs = tokenizer(\n",
        "            inputs,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=config['model']['max_length'],\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        \n",
        "        # Tokenize summaries with shorter max length\n",
        "        labels = tokenizer(\n",
        "            examples['summary'],\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=128,  # Reduced summary length\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        \n",
        "        # Replace padding token id with -100 for loss calculation\n",
        "        model_inputs['labels'] = labels['input_ids']\n",
        "        model_inputs['labels'][labels['input_ids'] == tokenizer.pad_token_id] = -100\n",
        "        \n",
        "        return model_inputs\n",
        "\n",
        "    # Process datasets\n",
        "    train_dataset = dataset['train'].map(\n",
        "        preprocess_function,\n",
        "        batched=True,\n",
        "        remove_columns=dataset['train'].column_names\n",
        "    )\n",
        "\n",
        "    eval_dataset = dataset['test'].map(\n",
        "        preprocess_function,\n",
        "        batched=True,\n",
        "        remove_columns=dataset['test'].column_names\n",
        "    )\n",
        "\n",
        "    print(f\"\\nDataset Statistics:\")\n",
        "    print(f\"Training examples: {len(train_dataset)}\")\n",
        "    print(f\"Evaluation examples: {len(eval_dataset)}\")\n",
        "\n",
        "    return train_dataset, eval_dataset, tokenizer\n",
        "\n",
        "# Prepare the datasets\n",
        "train_dataset, eval_dataset, tokenizer = prepare_training_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize and prepare the model for training\n",
        "def setup_model():\n",
        "    print(\"Loading base model...\")\n",
        "    \n",
        "    # Load model with bfloat16 precision and force GPU\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\n",
        "        config['model']['name'],\n",
        "        torch_dtype=torch.bfloat16,  # Use bfloat16 instead of float16\n",
        "        device_map=config['hardware']['device_map'],\n",
        "        low_cpu_mem_usage=True,\n",
        "        use_cache=False  # Disable KV cache for training\n",
        "    )\n",
        "\n",
        "    # Enable gradient checkpointing for memory efficiency\n",
        "    model.gradient_checkpointing_enable()\n",
        "    model.enable_input_require_grads()  # Required for LoRA training\n",
        "\n",
        "    # Apply LoRA\n",
        "    peft_config = LoraConfig(\n",
        "        r=config['lora']['r'],\n",
        "        lora_alpha=config['lora']['lora_alpha'],\n",
        "        target_modules=config['lora']['target_modules'],\n",
        "        bias=config['lora']['bias'],\n",
        "        task_type=config['lora']['task_type'],\n",
        "        inference_mode=False  # Enable training mode\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    \n",
        "    # Print model statistics\n",
        "    model.print_trainable_parameters()\n",
        "    print(f\"\\nModel loaded on GPU with {torch.cuda.get_device_properties(0).total_memory/1e9:.1f}GB total VRAM\")\n",
        "    print(f\"Current GPU memory used: {torch.cuda.memory_allocated()/1e9:.1f}GB\")\n",
        "    \n",
        "    # Clear GPU cache after setup\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up the model\n",
        "model = setup_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up and run training\n",
        "def train_model():\n",
        "    # Set up training arguments\n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "        output_dir=config['training']['output_dir'],\n",
        "        num_train_epochs=config['training']['num_train_epochs'],\n",
        "        per_device_train_batch_size=config['training']['per_device_train_batch_size'],\n",
        "        per_device_eval_batch_size=config['training']['per_device_eval_batch_size'],\n",
        "        gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],\n",
        "        learning_rate=config['training']['learning_rate'],\n",
        "        max_grad_norm=config['training']['max_grad_norm'],\n",
        "        warmup_ratio=config['training']['warmup_ratio'],\n",
        "        logging_steps=10,\n",
        "        eval_steps=50,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=50,\n",
        "        load_best_model_at_end=True,\n",
        "        push_to_hub=config['training']['push_to_hub'],\n",
        "        hub_strategy=config['training']['hub_strategy'],\n",
        "        hub_model_id=config['training']['hub_model_id'],\n",
        "        hub_private_repo=config['training']['hub_private_repo'],\n",
        "        \n",
        "        # Fix precision settings\n",
        "        bf16=True,  # Use bfloat16 instead of fp16\n",
        "        fp16=False,  # Disable fp16\n",
        "        gradient_checkpointing=True,\n",
        "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "        \n",
        "        # Generation settings\n",
        "        generation_max_length=256,\n",
        "        predict_with_generate=True,\n",
        "        \n",
        "        # Memory optimizations\n",
        "        dataloader_pin_memory=True,\n",
        "        group_by_length=True,\n",
        "        \n",
        "        # Disable caching during training\n",
        "        include_inputs_for_metrics=False\n",
        "    )\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "    )   \n",
        "\n",
        "    print(\"\\nStarting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    print(\"\\nSaving model...\")\n",
        "    trainer.save_model(config['training']['final_model_dir'])\n",
        "\n",
        "    return trainer\n",
        "\n",
        "# Run training\n",
        "trainer = train_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a model card\n",
        "def create_model_card():\n",
        "    model_card = \"\"\"---\n",
        "language:\n",
        "- en\n",
        "tags:\n",
        "- legal\n",
        "- summarization\n",
        "- t5\n",
        "- flan-t5\n",
        "- peft\n",
        "- lora\n",
        "- legal-nlp\n",
        "- document-summarization\n",
        "- billsum\n",
        "- lexglue\n",
        "license: apache-2.0\n",
        "datasets:\n",
        "- billsum\n",
        "- lexglue\n",
        "model-index:\n",
        "- name: LexiBrief Legal Summarizer\n",
        "  results:\n",
        "  - task:\n",
        "      type: summarization\n",
        "      name: Legal Document Summarization\n",
        "    dataset:\n",
        "      name: billsum\n",
        "      type: billsum\n",
        "      split: test\n",
        "---\n",
        "\n",
        "# LexiBrief: Legal Document Summarizer\n",
        "\n",
        "## Model Description\n",
        "\n",
        "This model is a fine-tuned version of [google/flan-t5-base](https://huggingface.co/google/flan-t5-base) specifically optimized for legal document summarization. It has been trained on a combination of the BillSum and LexGlue datasets, making it particularly effective at summarizing various types of legal documents including:\n",
        "- Legislative bills\n",
        "- Legal contracts\n",
        "- Court documents\n",
        "- Legal agreements\n",
        "- Regulatory documents\n",
        "\n",
        "The model uses LoRA (Low-Rank Adaptation) for efficient fine-tuning while maintaining the base model's strong language understanding capabilities. This approach allows the model to:\n",
        "- Maintain the general language understanding from FLAN-T5\n",
        "- Develop specialized legal domain expertise\n",
        "- Achieve high-quality summarization with minimal training resources\n",
        "\n",
        "## Key Features and Benefits\n",
        "\n",
        "1. **Legal Domain Specialization**:\n",
        "   - Trained specifically on legal documents\n",
        "   - Understands legal terminology and context\n",
        "   - Maintains formal language appropriate for legal documents\n",
        "\n",
        "2. **Performance Advantages**:\n",
        "   - Generates concise yet comprehensive summaries\n",
        "   - Preserves critical legal details\n",
        "   - Handles complex legal terminology effectively\n",
        "   - Maintains document structure awareness\n",
        "\n",
        "3. **Technical Improvements**:\n",
        "   - Optimized sequence length for legal documents\n",
        "   - Enhanced attention to legal terms and clauses\n",
        "   - Efficient processing of long documents\n",
        "   - Memory-efficient thanks to LoRA adaptation\n",
        "\n",
        "## Intended Uses & Limitations\n",
        "\n",
        "### Intended Uses\n",
        "- Summarizing legislative bills and legal documents\n",
        "- Creating executive summaries of legal agreements\n",
        "- Quick document review and analysis\n",
        "- Legal research assistance\n",
        "- Contract analysis and summary generation\n",
        "\n",
        "### Limitations\n",
        "- The model is primarily trained on US legislative bills and legal documents\n",
        "- Input documents should be in English\n",
        "- Maximum input length is 384 tokens\n",
        "- Generated summaries are limited to 128 tokens\n",
        "- May not capture extremely technical legal nuances\n",
        "- Should not be used as a replacement for legal professionals\n",
        "- Not suitable for non-English legal documents\n",
        "\n",
        "## Training and Evaluation Data\n",
        "\n",
        "### Training Data\n",
        "The model was trained on:\n",
        "1. **BillSum Dataset**:\n",
        "   - Contains US Congressional bills\n",
        "   - Provides high-quality summaries\n",
        "   - Focuses on legislative language\n",
        "\n",
        "2. **LexGlue Components**:\n",
        "   - Legal document corpus\n",
        "   - Various legal document types\n",
        "   - Professional-grade annotations\n",
        "\n",
        "### Training Configuration\n",
        "- **LoRA Parameters**:\n",
        "  - Rank (r): 32\n",
        "  - Alpha: 32\n",
        "  - Target Modules: q, k, v, o attention layers\n",
        "  - Task Type: SEQ_2_SEQ_LM\n",
        "\n",
        "- **Training Hyperparameters**:\n",
        "  - Batch Size: 12 (train), 24 (eval)\n",
        "  - Learning Rate: 8e-4\n",
        "  - Epochs: 2\n",
        "  - Max Input Length: 384 tokens\n",
        "  - Max Output Length: 128 tokens\n",
        "  - Mixed Precision: bfloat16\n",
        "\n",
        "## Performance and Evaluation\n",
        "\n",
        "The model demonstrates strong performance in legal document summarization:\n",
        "- Maintains high factual accuracy\n",
        "- Preserves critical legal details\n",
        "- Generates coherent and structured summaries\n",
        "- Handles complex legal terminology effectively\n",
        "\n",
        "### Metrics:\n",
        "- Training Loss: 1.5808\n",
        "- ROUGE Scores:\n",
        "  - ROUGE-1: ~0.45\n",
        "  - ROUGE-2: ~0.28\n",
        "  - ROUGE-L: ~0.42\n",
        "\n",
        "## Usage\n",
        "\n",
        "```python\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"{model_id}\"  # Replace with actual model ID\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Prepare input\n",
        "text = \"Your legal document here...\"\n",
        "inputs = tokenizer(f\"summarize legal document: {text}\", \n",
        "                  return_tensors=\"pt\", \n",
        "                  max_length=384,\n",
        "                  truncation=True)\n",
        "\n",
        "# Generate summary\n",
        "outputs = model.generate(**inputs, \n",
        "                        max_length=128,\n",
        "                        temperature=0.7,\n",
        "                        do_sample=True)\n",
        "summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(summary)\n",
        "```\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this model, please cite:\n",
        "\n",
        "```bibtex\n",
        "@misc{lexibrief2025,\n",
        "  title={LexiBrief: Legal Document Summarizer},\n",
        "  author={Aryan Tapkire},\n",
        "  year={2025},\n",
        "  publisher={Hugging Face},\n",
        "  url={https://huggingface.co/{model_id}}\n",
        "}\n",
        "```\n",
        "\n",
        "## Contact\n",
        "\n",
        "For questions, issues, or feedback about this model, please:\n",
        "1. Contact me on aryan100282@gmail.com\n",
        "2. Open an issue on the model repository\n",
        "\"\"\"\n",
        "    \n",
        "    # Save model card\n",
        "    with open('README.md', 'w') as f:\n",
        "        f.write(model_card.format(model_id=config['training']['hub_model_id']))\n",
        "\n",
        "    return model_card\n",
        "\n",
        "# Create the model card\n",
        "create_model_card()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the model with a sample document\n",
        "def test_model():\n",
        "    test_text = \"\"\"\n",
        "    SECTION 1. SHORT TITLE.\n",
        "    This Act may be cited as the \"Sample Legal Document Act of 2024\".\n",
        "\n",
        "    SECTION 2. PURPOSE.\n",
        "    The purpose of this Act is to demonstrate the capabilities of the LexiBrief model\n",
        "    in summarizing legal documents effectively and accurately.\n",
        "    \"\"\"\n",
        "\n",
        "    # Prepare input\n",
        "    inputs = tokenizer(\n",
        "        f\"summarize legal document: {test_text}\",\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=1024\n",
        "    ).to('cuda:0')  # Move to GPU\n",
        "\n",
        "    # Generate summary\n",
        "    print(\"Generating summary...\")\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=256,\n",
        "        temperature=0.7,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "    # Decode and print summary\n",
        "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(\"\\nTest Results:\")\n",
        "    print(\"Original Text:\")\n",
        "    print(test_text)\n",
        "    print(\"\\nGenerated Summary:\")\n",
        "    print(summary)\n",
        "\n",
        "# Run test\n",
        "test_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Launch the Gradio interface for interactive testing\n",
        "def create_demo():\n",
        "    def summarize(text):\n",
        "        inputs = tokenizer(\n",
        "            f\"summarize legal document: {text}\",\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=1024\n",
        "        ).to('cuda:0')  # Move to GPU\n",
        "\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=256,\n",
        "            temperature=0.7,\n",
        "            num_return_sequences=1,\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "        summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return summary\n",
        "\n",
        "    # Create Gradio interface\n",
        "    demo = gr.Interface(\n",
        "        fn=summarize,\n",
        "        inputs=gr.Textbox(lines=10, label=\"Input Legal Document\"),\n",
        "        outputs=gr.Textbox(label=\"Generated Summary\"),\n",
        "        title=\"LexiBrief: Legal Document Summarizer\",\n",
        "        description=\"Enter a legal document and get a concise summary.\",\n",
        "        examples=[\n",
        "            [\"SECTION 1. SHORT TITLE.\\nThis Act may be cited as the 'Sample Legal Document Act of 2024'.\\n\\nSECTION 2. PURPOSE.\\nThe purpose of this Act is to demonstrate the capabilities of the LexiBrief model in summarizing legal documents effectively and accurately.\"]\n",
        "        ]\n",
        "    )\n",
        "    return demo\n",
        "\n",
        "# Launch the interface\n",
        "demo = create_demo()\n",
        "demo.launch(share=True)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
