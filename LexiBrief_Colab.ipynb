{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# LexiBrief: AI-Powered Legal Document Summarizer\n",
        "\n",
        "This Colab notebook sets up and runs the LexiBrief project, which uses the Mistral-7B-Instruct model for legal document summarization. The notebook will:\n",
        "\n",
        "1. Set up the environment and dependencies\n",
        "2. Clone the project from GitHub\n",
        "3. Configure the model for CPU/GPU training\n",
        "4. Train and evaluate the model\n",
        "5. Launch the Gradio interface for testing\n",
        "\n",
        "Note: Make sure you're running this in a GPU-enabled Colab runtime for optimal performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if we're running in Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"Setting up Colab environment...\")\n",
        "    \n",
        "    # Install all required packages in one go to minimize installation time\n",
        "    !pip install -q torch --extra-index-url https://download.pytorch.org/whl/cu118 \\\n",
        "        transformers==4.30.2 \\\n",
        "        numpy==1.24.3 \\\n",
        "        pandas==2.0.3 \\\n",
        "        tqdm==4.65.0 \\\n",
        "        scikit-learn \\\n",
        "        gradio==3.40.1 \\\n",
        "        peft==0.4.0 \\\n",
        "        datasets==2.12.0 \\\n",
        "        accelerate==0.21.0 \\\n",
        "        bitsandbytes==0.41.0 \\\n",
        "        PyYAML==6.0\n",
        "    \n",
        "    print(\"\\nRestarting runtime to apply changes...\")\n",
        "    import IPython\n",
        "    IPython.Application.instance().kernel.do_shutdown(True)\n",
        "else:\n",
        "    print(\"Running in local environment\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify the installation and setup environment\n",
        "import torch\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "print(\"Checking environment setup...\")\n",
        "\n",
        "# Verify GPU availability\n",
        "print(\"\\nGPU Information:\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
        "    print(f\"Current GPU Memory Usage: {torch.cuda.memory_allocated(0)/1024**2:.2f} MB\")\n",
        "\n",
        "# Clone repository if it doesn't exist\n",
        "if not os.path.exists('lexibrief-ai'):\n",
        "    print(\"\\nCloning LexiBrief repository...\")\n",
        "    !git clone https://github.com/AryanT7/lexibrief-ai.git\n",
        "    %cd lexibrief-ai\n",
        "else:\n",
        "    print(\"\\nRepository already exists, updating...\")\n",
        "    %cd lexibrief-ai\n",
        "    !git pull\n",
        "\n",
        "print(\"\\nSetup completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import sys\n",
        "import yaml\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer, \n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import gradio as gr\n",
        "\n",
        "# Add the project root to Python path\n",
        "sys.path.append('.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure training settings based on available hardware\n",
        "def setup_training_config():\n",
        "    config_updates = {\n",
        "        'model': {\n",
        "            'name': 'mistralai/Mistral-7B-Instruct-v0.1',\n",
        "            'load_in_4bit': True,\n",
        "            'use_flash_attention': True\n",
        "        },\n",
        "        'training': {\n",
        "            'per_device_train_batch_size': 4 if torch.cuda.is_available() else 1,\n",
        "            'per_device_eval_batch_size': 4 if torch.cuda.is_available() else 1,\n",
        "            'gradient_accumulation_steps': 4,\n",
        "            'num_train_epochs': 3,\n",
        "            'learning_rate': 2e-4,\n",
        "            'max_grad_norm': 0.3,\n",
        "            'warmup_ratio': 0.03\n",
        "        },\n",
        "        'lora': {\n",
        "            'r': 64,\n",
        "            'lora_alpha': 16,\n",
        "            'target_modules': [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "            'bias': \"none\",\n",
        "            'task_type': \"CAUSAL_LM\"\n",
        "        },\n",
        "        'hardware': {\n",
        "            'mixed_precision': 'fp16' if torch.cuda.is_available() else 'no',\n",
        "            'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Create configs directory if it doesn't exist\n",
        "    os.makedirs('configs', exist_ok=True)\n",
        "    \n",
        "    # Save config\n",
        "    with open('configs/training_config.yaml', 'w') as f:\n",
        "        yaml.dump(config_updates, f, default_flow_style=False)\n",
        "    \n",
        "    return config_updates\n",
        "\n",
        "# Set up the configuration\n",
        "config = setup_training_config()\n",
        "print(\"\\nTraining Configuration:\")\n",
        "print(yaml.dump(config, default_flow_style=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and prepare the dataset\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "def prepare_training_data():\n",
        "    print(\"Loading billsum dataset...\")\n",
        "    dataset = load_dataset(\"billsum\")\n",
        "    \n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config['model']['name'])\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    def preprocess_function(examples):\n",
        "        # Combine summary and text with instruction\n",
        "        prompts = [\n",
        "            f\"Instruction: Summarize the following legal document concisely.\\n\\nDocument: {text}\\n\\nSummary:\"\n",
        "            for text in examples['text']\n",
        "        ]\n",
        "        \n",
        "        # Tokenize inputs and targets\n",
        "        model_inputs = tokenizer(prompts, truncation=True, padding=True, max_length=1024)\n",
        "        labels = tokenizer(examples['summary'], truncation=True, padding=True, max_length=256)\n",
        "        model_inputs['labels'] = labels['input_ids']\n",
        "        \n",
        "        return model_inputs\n",
        "    \n",
        "    # Process datasets\n",
        "    train_dataset = dataset['train'].map(\n",
        "        preprocess_function,\n",
        "        batched=True,\n",
        "        remove_columns=dataset['train'].column_names\n",
        "    )\n",
        "    \n",
        "    eval_dataset = dataset['test'].map(\n",
        "        preprocess_function,\n",
        "        batched=True,\n",
        "        remove_columns=dataset['test'].column_names\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nDataset Statistics:\")\n",
        "    print(f\"Training examples: {len(train_dataset)}\")\n",
        "    print(f\"Evaluation examples: {len(eval_dataset)}\")\n",
        "    \n",
        "    return train_dataset, eval_dataset, tokenizer\n",
        "\n",
        "# Prepare the datasets\n",
        "train_dataset, eval_dataset, tokenizer = prepare_training_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize and prepare the model for training\n",
        "def setup_model():\n",
        "    print(\"Loading base model...\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        config['model']['name'],\n",
        "        load_in_4bit=config['model']['load_in_4bit'],\n",
        "        device_map='auto',\n",
        "        torch_dtype=torch.float16 if config['hardware']['mixed_precision'] == 'fp16' else torch.float32\n",
        "    )\n",
        "    \n",
        "    # Prepare model for k-bit training\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "    \n",
        "    # Configure LoRA\n",
        "    peft_config = LoraConfig(\n",
        "        r=config['lora']['r'],\n",
        "        lora_alpha=config['lora']['lora_alpha'],\n",
        "        target_modules=config['lora']['target_modules'],\n",
        "        bias=config['lora']['bias'],\n",
        "        task_type=config['lora']['task_type']\n",
        "    )\n",
        "    \n",
        "    # Get PEFT model\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    \n",
        "    # Print trainable parameters\n",
        "    model.print_trainable_parameters()\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Set up the model\n",
        "model = setup_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up and run training\n",
        "def train_model():\n",
        "    # Set up training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        num_train_epochs=config['training']['num_train_epochs'],\n",
        "        per_device_train_batch_size=config['training']['per_device_train_batch_size'],\n",
        "        per_device_eval_batch_size=config['training']['per_device_eval_batch_size'],\n",
        "        gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],\n",
        "        learning_rate=config['training']['learning_rate'],\n",
        "        max_grad_norm=config['training']['max_grad_norm'],\n",
        "        warmup_ratio=config['training']['warmup_ratio'],\n",
        "        logging_steps=10,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=50,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=50,\n",
        "        load_best_model_at_end=True,\n",
        "        push_to_hub=False,\n",
        "    )\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    print(\"\\nStarting training...\")\n",
        "    trainer.train()\n",
        "    \n",
        "    print(\"\\nSaving model...\")\n",
        "    trainer.save_model(\"./final_model\")\n",
        "    \n",
        "    return trainer\n",
        "\n",
        "# Run training\n",
        "trainer = train_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the model with a sample document\n",
        "def test_model():\n",
        "    test_text = \"\"\"\n",
        "    SECTION 1. SHORT TITLE.\n",
        "    This Act may be cited as the \"Sample Legal Document Act of 2024\".\n",
        "    \n",
        "    SECTION 2. PURPOSE.\n",
        "    The purpose of this Act is to demonstrate the capabilities of the LexiBrief model\n",
        "    in summarizing legal documents effectively and accurately.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Prepare input\n",
        "    prompt = f\"Instruction: Summarize the following legal document concisely.\\n\\nDocument: {test_text}\\n\\nSummary:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "    \n",
        "    # Generate summary\n",
        "    print(\"Generating summary...\")\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.7,\n",
        "        num_return_sequences=1,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    \n",
        "    # Decode and print summary\n",
        "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(\"\\nTest Results:\")\n",
        "    print(\"Original Text:\")\n",
        "    print(test_text)\n",
        "    print(\"\\nGenerated Summary:\")\n",
        "    print(summary)\n",
        "\n",
        "# Run test\n",
        "test_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Launch the Gradio interface for interactive testing\n",
        "def create_demo():\n",
        "    def summarize(text):\n",
        "        prompt = f\"Instruction: Summarize the following legal document concisely.\\n\\nDocument: {text}\\n\\nSummary:\"\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
        "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "        \n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.7,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "        \n",
        "        summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return summary\n",
        "\n",
        "    # Create Gradio interface\n",
        "    demo = gr.Interface(\n",
        "        fn=summarize,\n",
        "        inputs=gr.Textbox(lines=10, label=\"Input Legal Document\"),\n",
        "        outputs=gr.Textbox(label=\"Generated Summary\"),\n",
        "        title=\"LexiBrief: Legal Document Summarizer\",\n",
        "        description=\"Enter a legal document and get a concise summary.\",\n",
        "        examples=[\n",
        "            [\"SECTION 1. SHORT TITLE.\\nThis Act may be cited as the 'Sample Legal Document Act of 2024'.\\n\\nSECTION 2. PURPOSE.\\nThe purpose of this Act is to demonstrate the capabilities of the LexiBrief model in summarizing legal documents effectively and accurately.\"]\n",
        "        ]\n",
        "    )\n",
        "    return demo\n",
        "\n",
        "# Launch the interface\n",
        "demo = create_demo()\n",
        "demo.launch(share=True)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
