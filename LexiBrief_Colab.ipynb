{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# LexiBrief: AI-Powered Legal Document Summarizer\n",
        "\n",
        "# This Colab notebook sets up and runs the LexiBrief project, which uses the Mistral-7B-Instruct model for legal document summarization. The notebook will:\n",
        "\n",
        "# 1. Set up the environment and dependencies\n",
        "# 2. Clone the project from GitHub\n",
        "# 3. Configure the model for CPU/GPU training\n",
        "# 4. Train and evaluate the model\n",
        "# 5. Launch the Gradio interface for testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up project structure and paths\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Check if we're in Colab\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    # Clone the repository if it doesn't exist\n",
        "    if not os.path.exists('LexiBrief'):\n",
        "        !git clone https://github.com/yourusername/LexiBrief.git\n",
        "    \n",
        "    # Change to the project directory\n",
        "    %cd LexiBrief\n",
        "\n",
        "# Define project paths\n",
        "PROJECT_ROOT = Path(os.getcwd())\n",
        "PATHS = {\n",
        "    'configs': PROJECT_ROOT / 'configs',\n",
        "    'data': PROJECT_ROOT / 'data',\n",
        "    'models': PROJECT_ROOT / 'models',\n",
        "    'outputs': PROJECT_ROOT / 'outputs',\n",
        "    'logs': PROJECT_ROOT / 'logs',\n",
        "    'final_model': PROJECT_ROOT / 'models' / 'final_model',\n",
        "    'results': PROJECT_ROOT / 'outputs' / 'results'\n",
        "}\n",
        "\n",
        "# Create all necessary directories\n",
        "for path in PATHS.values():\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"\\nProject structure created:\")\n",
        "for name, path in PATHS.items():\n",
        "    print(f\"{name}: {path}\")\n",
        "\n",
        "if IN_COLAB:\n",
        "    # Install project requirements\n",
        "    if os.path.exists('requirements.txt'):\n",
        "        !pip install -r requirements.txt\n",
        "    print(\"\\nProject dependencies installed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone and set up the project repository\n",
        "import os\n",
        "\n",
        "# Check if we're in Colab\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    # Clone the repository\n",
        "    !git clone https://github.com/yourusername/LexiBrief.git\n",
        "    \n",
        "    # Change to the project directory\n",
        "    %cd LexiBrief\n",
        "    \n",
        "    # Create necessary directories if they don't exist\n",
        "    os.makedirs(\"configs\", exist_ok=True)\n",
        "    os.makedirs(\"data\", exist_ok=True)\n",
        "    os.makedirs(\"models\", exist_ok=True)\n",
        "    os.makedirs(\"outputs\", exist_ok=True)\n",
        "    os.makedirs(\"logs\", exist_ok=True)\n",
        "    \n",
        "    print(\"\\nProject structure created:\")\n",
        "    !ls -la\n",
        "\n",
        "    # Install project requirements\n",
        "    !pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up the Colab environment\n",
        "import sys\n",
        "import os\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"Setting up Colab environment...\")\n",
        "\n",
        "    # Clean environment first\n",
        "    !pip uninstall -y numpy transformers torch torchvision torchaudio datasets accelerate -q\n",
        "    \n",
        "    # Install numpy 1.26.4 first\n",
        "    !pip install -q numpy==1.26.4\n",
        "    \n",
        "    # Install PyTorch and related packages\n",
        "    !pip install -q torch torchvision torchaudio\n",
        "    \n",
        "    # Install transformers and its dependencies\n",
        "    !pip install -q transformers==4.40.2\n",
        "    !pip install -q accelerate==0.26.0\n",
        "    !pip install -q datasets\n",
        "    !pip install -q peft==0.10.0\n",
        "    \n",
        "    # Install other dependencies\n",
        "    !pip install -q \"websockets>=10.0,<12.0\"\n",
        "    !pip install -q evaluate rouge_score nltk\n",
        "    !pip install -q wandb python-dotenv requests PyYAML scipy sentencepiece\n",
        "    !pip install -q gradio==3.40.1\n",
        "    \n",
        "    # Force reinstall numpy to ensure version\n",
        "    !pip install -q --force-reinstall numpy==1.26.4\n",
        "\n",
        "    # Confirm GPU\n",
        "    print(\"\\nVerifying GPU setup...\")\n",
        "    import torch\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
        "    else:\n",
        "        print(\"No GPU found. Using CPU.\")\n",
        "\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import sys\n",
        "import yaml\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import gradio as gr\n",
        "\n",
        "# Add the project root to Python path\n",
        "sys.path.append('.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update configuration with project paths\n",
        "def update_paths_in_config():\n",
        "    global config\n",
        "    \n",
        "    # Update training paths\n",
        "    config['training'].update({\n",
        "        'output_dir': str(PATHS['results']),\n",
        "        'logging_dir': str(PATHS['logs']),\n",
        "        'model_save_dir': str(PATHS['final_model'])\n",
        "    })\n",
        "    \n",
        "    # Save updated config\n",
        "    config_path = PATHS['configs'] / 'training_config.yaml'\n",
        "    with open(config_path, 'w') as f:\n",
        "        yaml.dump(config, f, default_flow_style=False)\n",
        "    \n",
        "    print(f\"Updated configuration saved to {config_path}\")\n",
        "    print(\"\\nUpdated paths:\")\n",
        "    print(f\"Output directory: {PATHS['results']}\")\n",
        "    print(f\"Model save directory: {PATHS['final_model']}\")\n",
        "    print(f\"Logs directory: {PATHS['logs']}\")\n",
        "\n",
        "# Update paths in configuration\n",
        "update_paths_in_config()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure training settings for T4 GPU\n",
        "def setup_training_config():\n",
        "    # T4 GPU has 15GB VRAM, we'll reserve 1GB for system\n",
        "    batch_size = 8  # Increased batch size for faster training\n",
        "    grad_accum = 8  # Reduced accumulation steps but maintain effective batch size of 64\n",
        "    \n",
        "    # Calculate optimal sequence lengths based on dataset\n",
        "    max_input_length = 384  # Reduced from 512 for faster processing\n",
        "    max_output_length = 96  # Reduced summary length for faster training\n",
        "    \n",
        "    config_updates = {\n",
        "        'model': {\n",
        "            'name': 'google/flan-t5-base',\n",
        "            'max_length': max_input_length,\n",
        "            'use_flash_attention': False  # FLAN-T5 doesn't support flash attention\n",
        "        },\n",
        "        'training': {\n",
        "            'per_device_train_batch_size': batch_size,\n",
        "            'per_device_eval_batch_size': batch_size * 2,  # Larger eval batch size for speed\n",
        "            'gradient_accumulation_steps': grad_accum,\n",
        "            'num_train_epochs': 2,  # Two epochs for better learning while keeping training time reasonable\n",
        "            'learning_rate': 2e-3,  # Higher learning rate for fast convergence but still stable learning\n",
        "            'max_grad_norm': 2.0,  # Allow larger gradients\n",
        "            'warmup_ratio': 0.01,  # Minimal warmup\n",
        "            'optim': 'adamw_torch',\n",
        "            'bf16': True,  # Use bfloat16 for better stability\n",
        "            'fp16': False,\n",
        "            'gradient_checkpointing': True,\n",
        "            'generation_max_length': max_output_length,\n",
        "            'predict_with_generate': True,\n",
        "            'gradient_checkpointing_kwargs': {\"use_reentrant\": False},\n",
        "            'logging_steps': 50,  # Reduced logging frequency\n",
        "            'eval_steps': 250,  # Less frequent evaluation\n",
        "            'save_steps': 250,  # Less frequent saving\n",
        "            'save_total_limit': 2,  # Keep only the last 2 checkpoints\n",
        "            'dataloader_num_workers': 4,  # Parallel data loading\n",
        "            'group_by_length': True,  # Reduces padding, increases speed\n",
        "        },\n",
        "        'lora': {\n",
        "            'r': 8,  # Reduced rank to save memory\n",
        "            'lora_alpha': 16,\n",
        "            'target_modules': [\"q\", \"k\", \"v\", \"o\"],  # T5 attention module names\n",
        "            'bias': \"none\",\n",
        "            'task_type': \"SEQ_2_SEQ_LM\",  # T5 is a seq2seq model\n",
        "            'inference_mode': False\n",
        "        },\n",
        "        'hardware': {\n",
        "            'mixed_precision': 'bf16',  # Match model's bfloat16 setting\n",
        "            'device_type': 'gpu',\n",
        "            'device': 'cuda',\n",
        "            'max_memory': {0: \"14GB\"},  # Reserve 1GB for system\n",
        "            'device_map': {\"\": 0},  # Map everything to GPU 0\n",
        "            'pin_memory': True,  # Pin memory for faster GPU transfer\n",
        "            'non_blocking': True  # Non-blocking GPU transfers\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Create configs directory if it doesn't exist\n",
        "    os.makedirs('configs', exist_ok=True)\n",
        "    \n",
        "    # Save config\n",
        "    with open('configs/training_config.yaml', 'w') as f:\n",
        "        yaml.dump(config_updates, f, default_flow_style=False)\n",
        "    \n",
        "    return config_updates\n",
        "\n",
        "# Set up the configuration\n",
        "config = setup_training_config()\n",
        "print(\"\\nTraining Configuration:\")\n",
        "print(yaml.dump(config, default_flow_style=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Login to Hugging Face\n",
        "from huggingface_hub import login\n",
        "login()\n",
        "#isithf_nkIwjkghPZIhMVdirYaTxgJQqUEGTpGVzGvJ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -U datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and prepare the dataset\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Initialize tokenizer globally (will be used by other functions)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "\n",
        "def prepare_training_data():\n",
        "    print(\"Loading and trimming billsum dataset...\")\n",
        "    dataset = load_dataset(\"billsum\", download_mode=\"force_redownload\", keep_in_memory=True)  # Keep in memory for faster processing\n",
        "    \n",
        "    # Use a moderate dataset size for better training while maintaining speed\n",
        "    max_train_samples = 2000  # Using more samples for better generalization\n",
        "    max_eval_samples = 400   # Increased proportionally with training samples\n",
        "    \n",
        "    # Filter out very long documents to speed up training\n",
        "    dataset = dataset.filter(\n",
        "        lambda x: len(x['text'].split()) < 1000,  # Only shorter documents\n",
        "        num_proc=4\n",
        "    )\n",
        "    \n",
        "    # Take subset of dataset\n",
        "    if len(dataset['train']) > max_train_samples:\n",
        "        dataset['train'] = dataset['train'].select(range(max_train_samples))\n",
        "    if len(dataset['test']) > max_eval_samples:\n",
        "        dataset['test'] = dataset['test'].select(range(max_eval_samples))\n",
        "    \n",
        "    def preprocess_function(examples):\n",
        "        # Format inputs for FLAN-T5 with task prefix\n",
        "        inputs = [\n",
        "            f\"Summarize the following legal document: {text}\"\n",
        "            for text in examples['text']\n",
        "        ]\n",
        "\n",
        "        # Tokenize inputs\n",
        "        model_inputs = tokenizer(\n",
        "            inputs,\n",
        "            max_length=config['model']['max_length'],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Tokenize summaries\n",
        "        labels = tokenizer(\n",
        "            examples['summary'],\n",
        "            max_length=config['training']['generation_max_length'],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Set up the labels\n",
        "        model_inputs['labels'] = labels['input_ids']\n",
        "        \n",
        "        # Replace padding token id with -100 for loss calculation\n",
        "        model_inputs['labels'][labels['input_ids'] == tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return model_inputs\n",
        "\n",
        "    # Process datasets\n",
        "    train_dataset = dataset['train'].map(\n",
        "        preprocess_function,\n",
        "        batched=True,\n",
        "        batch_size=16,  # Smaller batch size for GPU memory\n",
        "        remove_columns=dataset['train'].column_names,\n",
        "        num_proc=4  # Use multiple processes for CPU preprocessing\n",
        "    )\n",
        "\n",
        "    eval_dataset = dataset['test'].map(\n",
        "        preprocess_function,\n",
        "        batched=True,\n",
        "        batch_size=16,\n",
        "        remove_columns=dataset['test'].column_names,\n",
        "        num_proc=4\n",
        "    )\n",
        "\n",
        "    print(f\"\\nDataset Statistics:\")\n",
        "    print(f\"Training examples: {len(train_dataset)}\")\n",
        "    print(f\"Evaluation examples: {len(eval_dataset)}\")\n",
        "    print(f\"Input sequence length: {config['model']['max_length']}\")\n",
        "    print(f\"Output sequence length: {config['model']['max_length'] // 4}\")\n",
        "\n",
        "    return train_dataset, eval_dataset, tokenizer\n",
        "\n",
        "# Prepare the datasets\n",
        "train_dataset, eval_dataset, tokenizer = prepare_training_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize and prepare the model for training\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import torch\n",
        "\n",
        "def setup_model():\n",
        "    print(\"Loading FLAN-T5 base model...\")\n",
        "    \n",
        "    if not torch.cuda.is_available():\n",
        "        raise RuntimeError(\"This code requires a GPU to run\")\n",
        "    \n",
        "    # Load model with T4 GPU optimizations\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "        \"google/flan-t5-base\",\n",
        "        torch_dtype=torch.bfloat16,  # Use bfloat16 instead of float16\n",
        "        device_map={\"\": 0},  # Map everything to GPU 0, no auto-offloading\n",
        "        max_memory=config['hardware']['max_memory'],\n",
        "        load_in_8bit=False,  # We're using bf16, not 8-bit quantization\n",
        "        use_cache=False  # Disable KV cache for training\n",
        "    )\n",
        "\n",
        "    # Enable gradient checkpointing for memory efficiency\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "    # Apply LoRA configuration\n",
        "    peft_config = LoraConfig(\n",
        "        r=config['lora']['r'],\n",
        "        lora_alpha=config['lora']['lora_alpha'],\n",
        "        target_modules=config['lora']['target_modules'],\n",
        "        bias=config['lora']['bias'],\n",
        "        task_type=config['lora']['task_type'],\n",
        "        inference_mode=False\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(model, peft_config)\n",
        "\n",
        "    # Verify model is on GPU\n",
        "    if next(model.parameters()).device.type != 'cuda':\n",
        "        raise RuntimeError(\"Model failed to load on GPU\")\n",
        "\n",
        "    # Print model statistics\n",
        "    model.print_trainable_parameters()\n",
        "    print(f\"\\nModel loaded on GPU with {torch.cuda.get_device_properties(0).total_memory/1e9:.1f}GB total VRAM\")\n",
        "    print(f\"Current GPU memory used: {torch.cuda.memory_allocated()/1e9:.1f}GB\")\n",
        "    \n",
        "    return model  # Return only the model   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up the model\n",
        "model = setup_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up and run training\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "from transformers.trainer_utils import set_seed\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('tokenizers/punkt/english.pickle', quiet=True)\n",
        "\n",
        "# Verify NLTK data is downloaded\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    print(\"NLTK punkt tokenizer data successfully loaded\")\n",
        "except LookupError:\n",
        "    raise RuntimeError(\"Failed to download NLTK data. Please try running the cell again.\")\n",
        "\n",
        "def train_model():\n",
        "    # Set random seed for reproducibility\n",
        "    set_seed(42)\n",
        "    \n",
        "    # Load ROUGE metric for evaluation\n",
        "    rouge = evaluate.load('rouge')\n",
        "    \n",
        "    def compute_metrics(eval_pred):\n",
        "        predictions, labels = eval_pred\n",
        "        # Decode predictions\n",
        "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "        \n",
        "        # Replace -100 in the labels as we can't decode them\n",
        "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "        \n",
        "        # ROUGE expects newlines after each sentence\n",
        "        decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "        decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "        \n",
        "        # Compute ROUGE scores\n",
        "        result = rouge.compute(\n",
        "            predictions=decoded_preds, \n",
        "            references=decoded_labels, \n",
        "            use_stemmer=True\n",
        "        )\n",
        "        \n",
        "        # Extract scores\n",
        "        result = {key: value * 100 for key, value in result.items()}\n",
        "        return {k: round(v, 4) for k, v in result.items()}\n",
        "\n",
        "    # Set up training arguments\n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "        output_dir=str(PATHS['results']),\n",
        "        num_train_epochs=config['training']['num_train_epochs'],\n",
        "        per_device_train_batch_size=config['training']['per_device_train_batch_size'],\n",
        "        per_device_eval_batch_size=config['training']['per_device_eval_batch_size'],\n",
        "        gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],\n",
        "        learning_rate=config['training']['learning_rate'],\n",
        "        max_grad_norm=config['training']['max_grad_norm'],\n",
        "        warmup_ratio=config['training']['warmup_ratio'],\n",
        "        logging_steps=10,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=50,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=50,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"rouge2\",\n",
        "        greater_is_better=True,\n",
        "        push_to_hub=False,\n",
        "        \n",
        "        # GPU optimizations\n",
        "        bf16=config['training']['bf16'],  # Use bfloat16\n",
        "        fp16=False,  # Disable fp16 since we're using bf16\n",
        "        gradient_checkpointing=config['training']['gradient_checkpointing'],\n",
        "        dataloader_pin_memory=True,  # Pin memory for faster GPU transfer\n",
        "        dataloader_num_workers=4,  # Parallel data loading\n",
        "        group_by_length=True,  # Reduce padding, optimize GPU memory\n",
        "        \n",
        "        # Generation settings\n",
        "        predict_with_generate=True,\n",
        "        generation_max_length=config['training']['generation_max_length'],\n",
        "        generation_num_beams=4\n",
        "    )\n",
        "\n",
        "    # Initialize Seq2SeqTrainer\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "        data_collator=None  # Use default seq2seq collator\n",
        "    )\n",
        "\n",
        "    # Print GPU memory usage before training\n",
        "    print(f\"\\nGPU memory before training: {torch.cuda.memory_allocated()/1e9:.2f}GB\")\n",
        "    \n",
        "    print(\"\\nStarting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    print(\"\\nSaving model...\")\n",
        "    \n",
        "    \n",
        "    # Save the model and verify\n",
        "    model_save_path = str(PATHS['final_model'])\n",
        "    trainer.save_model(model_save_path)\n",
        "    \n",
        "    # Verify files were saved\n",
        "    print(\"\\nVerifying saved files:\")\n",
        "    if os.path.exists(model_save_path):\n",
        "        print(f\"Files in {model_save_path}:\")\n",
        "        for file in os.listdir(model_save_path):\n",
        "            file_size = os.path.getsize(os.path.join(model_save_path, file)) / (1024 * 1024)  # Size in MB\n",
        "            print(f\"- {file} ({file_size:.2f} MB)\")\n",
        "    else:\n",
        "        print(\"Warning: Model directory not found!\")\n",
        "    \n",
        "    # Print final GPU memory usage\n",
        "    print(f\"\\nFinal GPU memory usage: {torch.cuda.memory_allocated()/1e9:.2f}GB\")\n",
        "    \n",
        "    # Force save tokenizer and config\n",
        "    tokenizer.save_pretrained(model_save_path)\n",
        "    model.config.save_pretrained(model_save_path)\n",
        "\n",
        "    return trainer\n",
        "\n",
        "# Run training\n",
        "trainer = train_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify saved model files and structure\n",
        "import os\n",
        "import json\n",
        "\n",
        "def verify_model_files():\n",
        "    # Check main directories\n",
        "    directories = [str(PATHS['final_model']), str(PATHS['results']), str(PATHS['configs'])]\n",
        "    for dir_path in directories:\n",
        "        if os.path.exists(dir_path):\n",
        "            print(f\"\\n{dir_path}/ directory:\")\n",
        "            files = os.listdir(dir_path)\n",
        "            for file in files:\n",
        "                file_size = os.path.getsize(os.path.join(dir_path, file)) / (1024 * 1024)  # Size in MB\n",
        "                print(f\"- {file} ({file_size:.2f} MB)\")\n",
        "        else:\n",
        "            print(f\"\\nWarning: {dir_path}/ directory not found!\")\n",
        "    \n",
        "    # Check if model files exist\n",
        "    required_files = [\n",
        "        './final_model/adapter_config.json',\n",
        "        './final_model/adapter_model.bin',\n",
        "        './final_model/config.json',\n",
        "        './final_model/training_config.json',\n",
        "        './final_model/README.md'\n",
        "    ]\n",
        "    \n",
        "    print(\"\\nChecking required model files:\")\n",
        "    for file_path in required_files:\n",
        "        if os.path.exists(file_path):\n",
        "            file_size = os.path.getsize(file_path) / (1024 * 1024)  # Size in MB\n",
        "            print(f\"âœ“ {file_path} ({file_size:.2f} MB)\")\n",
        "        else:\n",
        "            print(f\"âœ— Missing: {file_path}\")\n",
        "    \n",
        "    # Try to load and verify training config\n",
        "    try:\n",
        "        with open('./final_model/training_config.json', 'r') as f:\n",
        "            training_config = json.load(f)\n",
        "            print(\"\\nTraining config verified âœ“\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"\\nWarning: training_config.json not found!\")\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"\\nWarning: training_config.json is not valid JSON!\")\n",
        "\n",
        "# Run verification\n",
        "verify_model_files()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare model for publishing\n",
        "def prepare_model_for_publishing():\n",
        "    import json\n",
        "    from datetime import datetime\n",
        "    \n",
        "    # 1. Create model card\n",
        "    model_card = f\"\"\"---\n",
        "language:\n",
        "- en\n",
        "tags:\n",
        "- legal-documents\n",
        "- summarization\n",
        "- flan-t5\n",
        "- lora\n",
        "license: apache-2.0\n",
        "datasets:\n",
        "- billsum\n",
        "metrics:\n",
        "- rouge\n",
        "model-index:\n",
        "- name: LexiBrief-FLAN-T5-Legal-Summarizer\n",
        "  results:\n",
        "  - task:\n",
        "      type: summarization\n",
        "      name: Legal Document Summarization\n",
        "    dataset:\n",
        "      type: billsum\n",
        "      name: BillSum\n",
        "      split: test\n",
        "      revision: None\n",
        "    metrics:\n",
        "    - type: rouge\n",
        "      value: {trainer.state.best_metric:.4f}\n",
        "      name: ROUGE-2\n",
        "---\n",
        "\n",
        "# LexiBrief: FLAN-T5 Legal Document Summarizer\n",
        "\n",
        "This model is a fine-tuned version of [google/flan-t5-base](https://huggingface.co/google/flan-t5-base) optimized for legal document summarization using LoRA.\n",
        "\n",
        "## Model Details\n",
        "\n",
        "- **Base Model**: FLAN-T5-base\n",
        "- **Task**: Legal Document Summarization\n",
        "- **Training Data**: BillSum dataset (filtered to {max_train_samples} samples)\n",
        "- **Training Time**: {datetime.now().strftime(\"%Y-%m-%d\")}\n",
        "- **Framework**: PyTorch with ðŸ¤— Transformers and PEFT\n",
        "\n",
        "## Training Procedure\n",
        "\n",
        "- **LoRA Configuration**:\n",
        "  - Rank: {config['lora']['r']}\n",
        "  - Alpha: {config['lora']['lora_alpha']}\n",
        "  - Target Modules: {config['lora']['target_modules']}\n",
        "\n",
        "- **Training Hyperparameters**:\n",
        "  - Learning Rate: {config['training']['learning_rate']}\n",
        "  - Batch Size: {config['training']['per_device_train_batch_size']}\n",
        "  - Gradient Accumulation: {config['training']['gradient_accumulation_steps']}\n",
        "  - Mixed Precision: bfloat16\n",
        "\n",
        "## Evaluation Results\n",
        "\n",
        "The model was evaluated on the BillSum test set ({max_eval_samples} samples):\n",
        "\n",
        "- ROUGE-2 Score: {trainer.state.best_metric:.4f}\n",
        "\n",
        "## Usage\n",
        "\n",
        "```python\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"your-username/lexibrief-legal-summarizer\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"your-username/lexibrief-legal-summarizer\")\n",
        "\n",
        "# Example usage\n",
        "text = \"Your legal document here...\"\n",
        "inputs = tokenizer(f\"Summarize the following legal document: {text}\", return_tensors=\"pt\", max_length=384, truncation=True)\n",
        "outputs = model.generate(**inputs, max_length=96, min_length=30, num_beams=4)\n",
        "summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "```\n",
        "\n",
        "## Limitations\n",
        "\n",
        "- The model was trained on a subset of the BillSum dataset\n",
        "- Best suited for documents under 1000 words\n",
        "- May not perform as well on non-legislative legal documents\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this model, please cite:\n",
        "\n",
        "```bibtex\n",
        "@misc{lexibrief2024,\n",
        "  title={LexiBrief: FLAN-T5 Legal Document Summarizer},\n",
        "  author={Your Name},\n",
        "  year={2024},\n",
        "  publisher={Hugging Face},\n",
        "  url={https://huggingface.co/your-username/lexibrief-legal-summarizer}\n",
        "}\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "    # Save model card\n",
        "    model_card_path = PATHS['final_model'] / 'README.md'\n",
        "    with open(model_card_path, 'w') as f:\n",
        "        f.write(model_card)\n",
        "    \n",
        "    # Save training config\n",
        "    config_path = PATHS['final_model'] / 'training_config.json'\n",
        "    with open(config_path, 'w') as f:\n",
        "        json.dump(config, f, indent=2)\n",
        "    \n",
        "    # Save evaluation results\n",
        "    eval_results = {\n",
        "        'best_rouge2': trainer.state.best_metric,\n",
        "        'training_steps': trainer.state.global_step,\n",
        "        'eval_samples': max_eval_samples,\n",
        "        'train_samples': max_train_samples\n",
        "    }\n",
        "    eval_results_path = PATHS['final_model'] / 'eval_results.json'\n",
        "    with open(eval_results_path, 'w') as f:\n",
        "        json.dump(eval_results, f, indent=2)\n",
        "    \n",
        "    print(f\"\\nModel artifacts saved in {PATHS['final_model']}:\")\n",
        "    print(\"- Model weights and configuration\")\n",
        "    print(\"- README.md (model card)\")\n",
        "    print(\"- training_config.json\")\n",
        "    print(\"- eval_results.json\")\n",
        "\n",
        "# Generate publishing materials\n",
        "prepare_model_for_publishing()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the model with a sample document\n",
        "def test_model():\n",
        "    test_text = \"\"\"\n",
        "    SECTION 1. SHORT TITLE.\n",
        "    This Act may be cited as the \"Sample Legal Document Act of 2024\".\n",
        "\n",
        "    SECTION 2. PURPOSE.\n",
        "    The purpose of this Act is to demonstrate the capabilities of the LexiBrief model\n",
        "    in summarizing legal documents effectively and accurately.\n",
        "    \"\"\"\n",
        "\n",
        "    # Prepare input for T5\n",
        "    input_text = f\"summarize legal document: {test_text}\"\n",
        "    inputs = tokenizer(\n",
        "        input_text, \n",
        "        return_tensors=\"pt\",\n",
        "        max_length=config['model']['max_length'],\n",
        "        padding=True,\n",
        "        truncation=True\n",
        "    )\n",
        "    # Move inputs to GPU\n",
        "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "\n",
        "    # Generate summary\n",
        "    print(\"Generating summary...\")\n",
        "    with torch.cuda.amp.autocast():  # Use mixed precision for inference\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=config['model']['max_length'] // 4,\n",
        "            min_length=30,  # Ensure summary isn't too short\n",
        "            num_beams=4,\n",
        "            length_penalty=2.0,  # Encourage longer summaries\n",
        "            temperature=0.7,\n",
        "            no_repeat_ngram_size=3,  # Avoid repetition\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "    # Decode and print summary\n",
        "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(\"\\nTest Results:\")\n",
        "    print(\"Original Text:\")\n",
        "    print(test_text)\n",
        "    print(\"\\nGenerated Summary:\")\n",
        "    print(summary)\n",
        "\n",
        "    # Print memory usage\n",
        "    print(f\"\\nCurrent GPU memory: {torch.cuda.memory_allocated()/1e9:.2f}GB\")\n",
        "    \n",
        "    # Clear GPU cache\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Run test\n",
        "test_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Launch the Gradio interface for interactive testing\n",
        "def create_demo():\n",
        "    def summarize(text):\n",
        "        # Prepare input for T5\n",
        "        input_text = f\"summarize legal document: {text}\"\n",
        "        inputs = tokenizer(\n",
        "            input_text,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=config['model']['max_length'],\n",
        "            padding=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        # Move inputs to GPU\n",
        "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "\n",
        "        # Generate summary with beam search\n",
        "        with torch.cuda.amp.autocast():  # Use mixed precision for inference\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_length=config['model']['max_length'] // 4,\n",
        "                min_length=30,\n",
        "                num_beams=4,\n",
        "                length_penalty=2.0,\n",
        "                temperature=0.7,\n",
        "                no_repeat_ngram_size=3,\n",
        "                early_stopping=True\n",
        "            )\n",
        "\n",
        "        # Decode summary\n",
        "        summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        \n",
        "        # Clear GPU cache after generation\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "        return summary\n",
        "\n",
        "    # Create Gradio interface with improved styling\n",
        "    demo = gr.Interface(\n",
        "        fn=summarize,\n",
        "        inputs=gr.Textbox(\n",
        "            lines=10,\n",
        "            label=\"Input Legal Document\",\n",
        "            placeholder=\"Paste your legal document here...\",\n",
        "            elem_id=\"input-box\"\n",
        "        ),\n",
        "        outputs=gr.Textbox(\n",
        "            label=\"Generated Summary\",\n",
        "            elem_id=\"output-box\"\n",
        "        ),\n",
        "        title=\"LexiBrief: Legal Document Summarizer (FLAN-T5)\",\n",
        "        description=\"\"\"\n",
        "        This tool uses the FLAN-T5 model fine-tuned on legal documents to generate concise summaries.\n",
        "        Enter a legal document and get a clear, accurate summary optimized for legal text.\n",
        "        \"\"\",\n",
        "        examples=[\n",
        "            [\"SECTION 1. SHORT TITLE.\\nThis Act may be cited as the 'Sample Legal Document Act of 2024'.\\n\\nSECTION 2. PURPOSE.\\nThe purpose of this Act is to demonstrate the capabilities of the LexiBrief model in summarizing legal documents effectively and accurately.\"]\n",
        "        ],\n",
        "        theme=\"default\",\n",
        "        css=\"\"\"\n",
        "        #input-box { min-height: 200px; }\n",
        "        #output-box { min-height: 100px; }\n",
        "        \"\"\"\n",
        "    )\n",
        "    return demo\n",
        "\n",
        "# Launch the interface\n",
        "demo = create_demo()\n",
        "demo.launch(share=True, enable_queue=True)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
