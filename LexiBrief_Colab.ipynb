{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# LexiBrief: AI-Powered Legal Document Summarizer\n",
        "\n",
        "# This Colab notebook sets up and runs the LexiBrief project, which uses the Mistral-7B-Instruct model for legal document summarization. The notebook will:\n",
        "\n",
        "# 1. Set up the environment and dependencies\n",
        "# 2. Clone the project from GitHub\n",
        "# 3. Configure the model for CPU/GPU training\n",
        "# 4. Train and evaluate the model\n",
        "# 5. Launch the Gradio interface for testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up the Colab environment\n",
        "import sys\n",
        "import os\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"Setting up Colab environment...\")\n",
        "\n",
        "    # Fix binary compatibility issues first\n",
        "    !pip uninstall -y numpy\n",
        "    !pip install -q numpy==1.26.4\n",
        "\n",
        "    # Install dependencies in safe order\n",
        "    !pip install -q \"websockets>=10.0,<12.0\"\n",
        "    !pip install -q gradio==3.40.1\n",
        "    !pip install -q transformers==4.40.2 pandas==2.2.0 tqdm peft==0.10.0 datasets accelerate bitsandbytes \\\n",
        "        evaluate rouge_score nltk wandb python-dotenv requests PyYAML scipy sentencepiece\n",
        "\n",
        "    # Clone or pull repo\n",
        "    if not os.path.exists('lexibrief-ai'):\n",
        "        print(\"\\nCloning LexiBrief repository...\")\n",
        "        !git clone https://github.com/AryanT7/lexibrief-ai.git\n",
        "    else:\n",
        "        print(\"\\nRepository already exists, updating...\")\n",
        "        %cd lexibrief-ai\n",
        "        !git pull\n",
        "        %cd ..\n",
        "\n",
        "    # Confirm GPU\n",
        "    print(\"\\nVerifying GPU setup...\")\n",
        "    import torch\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
        "    else:\n",
        "        print(\"No GPU found. Using CPU.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up broken install\n",
        "!pip uninstall -y transformers\n",
        "!rm -rf /usr/local/lib/python3.11/dist-packages/transformers\n",
        "!rm -rf /usr/local/lib/python3.11/dist-packages/transformers-*\n",
        "!rm -rf /content/transformers\n",
        "\n",
        "!pip install -q transformers==4.40.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import sys\n",
        "import yaml\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import gradio as gr\n",
        "\n",
        "# Add the project root to Python path\n",
        "sys.path.append('.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure training settings based on available hardware\n",
        "def setup_training_config():\n",
        "    config_updates = {\n",
        "        'model': {\n",
        "            'name': 'mistralai/Mistral-7B-Instruct-v0.1',\n",
        "            'load_in_4bit': True,\n",
        "            'use_flash_attention': True\n",
        "        },\n",
        "        'training': {\n",
        "            'per_device_train_batch_size': 4 if torch.cuda.is_available() else 1,\n",
        "            'per_device_eval_batch_size': 4 if torch.cuda.is_available() else 1,\n",
        "            'gradient_accumulation_steps': 4,\n",
        "            'num_train_epochs': 3,\n",
        "            'learning_rate': 2e-4,\n",
        "            'max_grad_norm': 0.3,\n",
        "            'warmup_ratio': 0.03\n",
        "        },\n",
        "        'lora': {\n",
        "            'r': 64,\n",
        "            'lora_alpha': 16,\n",
        "            'target_modules': [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "            'bias': \"none\",\n",
        "            'task_type': \"CAUSAL_LM\"\n",
        "        },\n",
        "        'hardware': {\n",
        "            'mixed_precision': 'fp16' if torch.cuda.is_available() else 'no',\n",
        "            'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Create configs directory if it doesn't exist\n",
        "    os.makedirs('configs', exist_ok=True)\n",
        "    \n",
        "    # Save config\n",
        "    with open('configs/training_config.yaml', 'w') as f:\n",
        "        yaml.dump(config_updates, f, default_flow_style=False)\n",
        "    \n",
        "    return config_updates\n",
        "\n",
        "# Set up the configuration\n",
        "config = setup_training_config()\n",
        "print(\"\\nTraining Configuration:\")\n",
        "print(yaml.dump(config, default_flow_style=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Login to Hugging Face\n",
        "from huggingface_hub import login\n",
        "login()\n",
        "#isithf_nkIwjkghPZIhMVdirYaTxgJQqUEGTpGVzGvJ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -U datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and prepare the dataset\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "def prepare_training_data():\n",
        "    print(\"Loading billsum dataset...\")\n",
        "    dataset = load_dataset(\"billsum\", download_mode=\"force_redownload\", keep_in_memory=True)\n",
        "    \n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config['model']['name'])\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    def preprocess_function(examples):\n",
        "        # Combine summary and text with instruction\n",
        "        prompts = [\n",
        "            f\"Instruction: Summarize the following legal document concisely.\\n\\nDocument: {text}\\n\\nSummary:\"\n",
        "            for text in examples['text']\n",
        "        ]\n",
        "        \n",
        "        # Tokenize inputs and targets\n",
        "        model_inputs = tokenizer(prompts, truncation=True, padding=True, max_length=1024)\n",
        "        labels = tokenizer(examples['summary'], truncation=True, padding=True, max_length=256)\n",
        "        model_inputs['labels'] = labels['input_ids']\n",
        "        \n",
        "        return model_inputs\n",
        "    \n",
        "    # Process datasets\n",
        "    train_dataset = dataset['train'].map(\n",
        "        preprocess_function,\n",
        "        batched=True,\n",
        "        remove_columns=dataset['train'].column_names\n",
        "    )\n",
        "    \n",
        "    eval_dataset = dataset['test'].map(\n",
        "        preprocess_function,\n",
        "        batched=True,\n",
        "        remove_columns=dataset['test'].column_names\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nDataset Statistics:\")\n",
        "    print(f\"Training examples: {len(train_dataset)}\")\n",
        "    print(f\"Evaluation examples: {len(eval_dataset)}\")\n",
        "    \n",
        "    return train_dataset, eval_dataset, tokenizer\n",
        "\n",
        "# Prepare the datasets\n",
        "train_dataset, eval_dataset, tokenizer = prepare_training_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize and prepare the model for training\n",
        "def setup_model():\n",
        "    print(\"Loading base model...\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        config['model']['name'],\n",
        "        load_in_4bit=config['model']['load_in_4bit'],\n",
        "        device_map='auto',\n",
        "        torch_dtype=torch.float16 if config['hardware']['mixed_precision'] == 'fp16' else torch.float32\n",
        "    )\n",
        "    \n",
        "    # Prepare model for k-bit training\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "    \n",
        "    # Configure LoRA\n",
        "    peft_config = LoraConfig(\n",
        "        r=config['lora']['r'],\n",
        "        lora_alpha=config['lora']['lora_alpha'],\n",
        "        target_modules=config['lora']['target_modules'],\n",
        "        bias=config['lora']['bias'],\n",
        "        task_type=config['lora']['task_type']\n",
        "    )\n",
        "    \n",
        "    # Get PEFT model\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    \n",
        "    # Print trainable parameters\n",
        "    model.print_trainable_parameters()\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Set up the model\n",
        "model = setup_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up and run training\n",
        "def train_model():\n",
        "    # Set up training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        num_train_epochs=config['training']['num_train_epochs'],\n",
        "        per_device_train_batch_size=config['training']['per_device_train_batch_size'],\n",
        "        per_device_eval_batch_size=config['training']['per_device_eval_batch_size'],\n",
        "        gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],\n",
        "        learning_rate=config['training']['learning_rate'],\n",
        "        max_grad_norm=config['training']['max_grad_norm'],\n",
        "        warmup_ratio=config['training']['warmup_ratio'],\n",
        "        logging_steps=10,\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=50,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=50,\n",
        "        load_best_model_at_end=True,\n",
        "        push_to_hub=False,\n",
        "    )\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    print(\"\\nStarting training...\")\n",
        "    trainer.train()\n",
        "    \n",
        "    print(\"\\nSaving model...\")\n",
        "    trainer.save_model(\"./final_model\")\n",
        "    \n",
        "    return trainer\n",
        "\n",
        "# Run training\n",
        "trainer = train_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the model with a sample document\n",
        "def test_model():\n",
        "    test_text = \"\"\"\n",
        "    SECTION 1. SHORT TITLE.\n",
        "    This Act may be cited as the \"Sample Legal Document Act of 2024\".\n",
        "    \n",
        "    SECTION 2. PURPOSE.\n",
        "    The purpose of this Act is to demonstrate the capabilities of the LexiBrief model\n",
        "    in summarizing legal documents effectively and accurately.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Prepare input\n",
        "    prompt = f\"Instruction: Summarize the following legal document concisely.\\n\\nDocument: {test_text}\\n\\nSummary:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "    \n",
        "    # Generate summary\n",
        "    print(\"Generating summary...\")\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.7,\n",
        "        num_return_sequences=1,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    \n",
        "    # Decode and print summary\n",
        "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(\"\\nTest Results:\")\n",
        "    print(\"Original Text:\")\n",
        "    print(test_text)\n",
        "    print(\"\\nGenerated Summary:\")\n",
        "    print(summary)\n",
        "\n",
        "# Run test\n",
        "test_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Launch the Gradio interface for interactive testing\n",
        "def create_demo():\n",
        "    def summarize(text):\n",
        "        prompt = f\"Instruction: Summarize the following legal document concisely.\\n\\nDocument: {text}\\n\\nSummary:\"\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
        "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "        \n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.7,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "        \n",
        "        summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return summary\n",
        "\n",
        "    # Create Gradio interface\n",
        "    demo = gr.Interface(\n",
        "        fn=summarize,\n",
        "        inputs=gr.Textbox(lines=10, label=\"Input Legal Document\"),\n",
        "        outputs=gr.Textbox(label=\"Generated Summary\"),\n",
        "        title=\"LexiBrief: Legal Document Summarizer\",\n",
        "        description=\"Enter a legal document and get a concise summary.\",\n",
        "        examples=[\n",
        "            [\"SECTION 1. SHORT TITLE.\\nThis Act may be cited as the 'Sample Legal Document Act of 2024'.\\n\\nSECTION 2. PURPOSE.\\nThe purpose of this Act is to demonstrate the capabilities of the LexiBrief model in summarizing legal documents effectively and accurately.\"]\n",
        "        ]\n",
        "    )\n",
        "    return demo\n",
        "\n",
        "# Launch the interface\n",
        "demo = create_demo()\n",
        "demo.launch(share=True)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
